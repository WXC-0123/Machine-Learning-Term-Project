{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS613 Final Project\n",
    "## Group 9: Stroke Prediction Model\n",
    "## Authors: Danny Li - Tien Nguyen - Emily Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_branch(var, y):\n",
    "    br = []\n",
    "    for i in range(len(var)):\n",
    "        if var.ndim ==2:\n",
    "            num = var[0,0]\n",
    "            #print(num)\n",
    "        else:\n",
    "            num = var[i]\n",
    "        if num not in br:\n",
    "            br.append(num)\n",
    "    #print(br)\n",
    "    class_num = []\n",
    "    for i in range(len(y)):\n",
    "        if y[i] not in class_num:\n",
    "            class_num.append(y[i])\n",
    "        \n",
    "        d_br = defaultdict()\n",
    "    for item in br:\n",
    "        d_br[item] = dict.fromkeys(class_num, 0)\n",
    "    for ele in range(len(y)):\n",
    "        for i in br:\n",
    "            for j in class_num:\n",
    "                if var[ele]==i and y[ele] == j:\n",
    "                    d_br[i][j]+=1\n",
    "    return d_br\n",
    "\n",
    "def entropy_ind(num, total):\n",
    "    \"\"\"\n",
    "    This function return the entropy value given the ratio of the attribute\n",
    "    \"\"\"\n",
    "    if num ==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (-num/total)*np.log2(num/total)\n",
    "\n",
    "# This function calculate entropy of a branch\n",
    "#input: a dict\n",
    "def entropy_br(d_br):\n",
    "    br_en = defaultdict()\n",
    "    keys = list(d_br.keys())\n",
    "    total = np.sum(list(d_br.values()))\n",
    "    for item in keys:\n",
    "        br_en[item] = 0\n",
    "    for i in range(len(keys)):\n",
    "        br_en[keys[i]] += entropy_ind(d_br[keys[i]],total)\n",
    "    en= np.sum(list(br_en.values()))\n",
    "    return [en, total]\n",
    "\n",
    "# This function calculate the entropy of a feature\n",
    "#entropy and count could be an array\n",
    "def entropy_var(entropy, count):\n",
    "    en_var = 0\n",
    "    total = np.sum(count)\n",
    "    if total ==0:\n",
    "        return entropy\n",
    "    for i in range(len(entropy)):\n",
    "        en_var += (count[i]/total)*entropy[i]\n",
    "    return en_var\n",
    "\n",
    "# This function return the feature that has min entropy\n",
    "def top12features(dataset, header):\n",
    "    en_all = []\n",
    "    en_dict = defaultdict()\n",
    "    for i in range(dataset.shape[1]-1):\n",
    "        en_ls = []\n",
    "        count_ls = []\n",
    "        d_br = count_branch(dataset[:,i],dataset[:,-1])\n",
    "        for key in d_br.keys():\n",
    "            ind_en, ind_count = entropy_br(d_br[key])\n",
    "            en_ls.append(ind_en)\n",
    "            count_ls.append(ind_count)\n",
    "        en_avg = entropy_var(np.array(en_ls), np.array(count_ls))\n",
    "        en_dict[i]=en_avg\n",
    "    en_sorted = sorted(en_dict.items(), key=lambda item:item[1])\n",
    "    top12 = en_sorted[:12]\n",
    "    ls_index = sorted([item[0] for item in top12])\n",
    "    d = defaultdict()\n",
    "    for item in ls_index:\n",
    "        d[header[item]]=dataset[:,item]\n",
    "    d[header[-1]] = dataset[:, -1]\n",
    "    df = pd.DataFrame(data=d)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (31) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s_data = pd.read_csv('data.csv')\n",
    "df = s_data.dropna(axis='columns')\n",
    "f_index = list(range(2,20))\n",
    "f_index.append(52)\n",
    "df_initial = s_data.iloc[:,f_index]\n",
    "df_initial = df_initial.dropna()\n",
    "df_initial=df_initial.drop(df_initial[df_initial['RDEF1']=='C'].index)\n",
    "df_initial=df_initial.drop(df_initial[df_initial['RDEF2']=='C'].index)\n",
    "df_initial=df_initial.drop(df_initial[df_initial['RDEF3']=='C'].index)\n",
    "df_initial=df_initial.drop(df_initial[df_initial['RDEF4']=='C'].index)\n",
    "df_initial=df_initial.drop(df_initial[df_initial['RDEF5']=='C'].index)\n",
    "df_initial=df_initial.drop(df_initial[df_initial['RDEF6']=='C'].index)\n",
    "df_initial=df_initial.drop(df_initial[df_initial['RDEF7']=='C'].index)\n",
    "df_initial=df_initial.drop(df_initial[df_initial['RDEF8']=='C'].index)\n",
    "df_initial=df_initial.drop(df_initial[df_initial['DNOSTRK']=='U'].index)\n",
    "\n",
    "df_initial.SEX = df_initial.SEX.replace({'M':0,'F':1}).astype(np.uint8)\n",
    "df_initial = df_initial.replace({'Y':1,'N':0})\n",
    "df_initial.RCONSC = df_initial.RCONSC.replace({'F':0,'D':1, 'U':2}).astype(np.uint8)\n",
    "df_initial['DNOSTRK'] = df_initial['DNOSTRK'].replace([0,1],[1,0])\n",
    "\n",
    "plt.rcParams['font.sans-serif']=['SimHei'] \n",
    "plt.figure(figsize=(6,9))\n",
    "labels = [u'stroke',u'no-stroke'] \n",
    "sizes = [12767, 312]\n",
    "colors = ['blue','red'] \n",
    "explode = (0,0)\n",
    "patches,text1,text2 = plt.pie(sizes,\n",
    "                      explode=explode,\n",
    "                      labels=labels,\n",
    "                      colors=colors,\n",
    "                      autopct = '%3.2f%%', \n",
    "                      shadow = False, \n",
    "                      startangle =90, \n",
    "                      pctdistance = 0.6)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_features = df_initial[['AGE','SEX','RSLEEP','RATRIAL', 'RVISINF', 'RDEF1', 'RDEF2', 'RDEF3', 'RDEF4', 'RDEF5', 'RDEF6', 'RDEF7','DNOSTRK']]\n",
    "top12_features = top12features(np.array(df_initial), list(df_initial.columns))\n",
    "\n",
    "p_nstrk = paper_features[paper_features['DNOSTRK']==0]\n",
    "p_strk = (paper_features[paper_features['DNOSTRK']==1]).sample(n = len(p_nstrk))\n",
    "p_balanced_data = pd.concat([p_strk, p_nstrk])\n",
    "\n",
    "t_nstrk = top12_features[top12_features['DNOSTRK']==0]\n",
    "t_strk = (top12_features[top12_features['DNOSTRK']==1]).sample(n = len(t_nstrk))\n",
    "t_balanced_data = pd.concat([t_strk, t_nstrk])\n",
    "\n",
    "df_initial.to_csv('18features.csv', header = True, sep = \",\", index = False)\n",
    "paper_features.to_csv('12paper_features.csv', header = True, sep = \",\", index = False)\n",
    "top12_features.to_csv('12top_features.csv', header = True, sep = \",\", index = False)\n",
    "p_balanced_data.to_csv('12paper_features_balanced.csv', header = True, sep = \",\", index = False)\n",
    "t_balanced_data.to_csv('12top_features_balanced.csv', header = True, sep = \",\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Machine Learning Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_pr(th, y_hat_val, y_val): #thread\n",
    "    # count TP, FP, FN\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    \n",
    "    for i in range(len(y_hat_val)):\n",
    "        if y_hat_val[i] >= th and y_val[i] == 1:\n",
    "            TP += 1\n",
    "        elif y_hat_val[i][0] >= th and y_val[i][0] == 0:\n",
    "            FP += 1\n",
    "        elif y_hat_val[i][0] < th and y_val[i][0] == 1:\n",
    "            FN += 1\n",
    "        else:\n",
    "            TN +=1\n",
    "    if TP == 0:\n",
    "        return 0,0\n",
    "    pre = TP/(TP+FP)\n",
    "    rec = TP/(TP+FN)\n",
    "    f_measure = 2*pre*rec/(pre+rec)\n",
    "    accuracy = (TP + TN)/(TP+TN+FP+FN)\n",
    "    return pre, rec, f_measure, accuracy\n",
    "\n",
    "def Logistic_Regression(filename):\n",
    "    data= pd.read_csv(filename)\n",
    "    spam_rand = data.sample(frac=1, random_state=0)\n",
    "\n",
    "    num_train = math.ceil(((2/3)*len(spam_rand)))\n",
    "    num_valid = len(spam_rand)-num_train\n",
    "\n",
    "    train = spam_rand.iloc[0:num_train,:] \n",
    "    val = spam_rand.iloc[num_train:,:]\n",
    "\n",
    "    X_train = np.ones(num_train).reshape(-1,1)\n",
    "    # zscore the training data\n",
    "    for j in range(len(train.columns)-1): #of features\n",
    "        vector = train.iloc[:,j].to_numpy()\n",
    "        mean = np.mean(vector)\n",
    "        std = np.std(vector,ddof=1)\n",
    "        stand = (vector-mean)/std\n",
    "        X_train = np.append(X_train,stand.reshape(-1,1),axis=1)\n",
    "\n",
    "    # add bias feature   validation part\n",
    "    X_val = np.ones(num_valid).reshape(-1,1) # // validation\n",
    "\n",
    "    # zscore the validation data\n",
    "    for j in range(len(train.columns)-1):\n",
    "        vector_val = val.iloc[:,j].to_numpy()\n",
    "        vector = train.iloc[:,j].to_numpy()\n",
    "        mean = np.mean(vector)\n",
    "        std = np.std(vector,ddof=1)\n",
    "        stand = (vector_val-mean)/std\n",
    "        X_val = np.append(X_val,stand.reshape(-1,1),axis=1)\n",
    "\n",
    "    np.random.seed(0)\n",
    "    omega = np.random.uniform(-0.01,0.01,X_train.shape[1]).reshape(-1,1)\n",
    "\n",
    "    y_hat = 1 / (1 + np.exp(-X_train.dot(omega)))\n",
    "    y_train = train.iloc[:,-1].to_numpy().reshape(-1,1)\n",
    "    j = np.mean(y_train*(np.log(y_hat))+(1-y_train)*(np.log(1-y_hat)))\n",
    "    y_val = val.iloc[:,-1].to_numpy().reshape(-1,1)\n",
    "    y_hat_val = 1 / (1 + np.exp(-X_val.dot(omega)))\n",
    "    j_val = np.mean(y_val*(np.log(y_hat_val))+(1-y_val)*(np.log(1-y_hat_val)))\n",
    "\n",
    "    # terminate until meet criteria\n",
    "    count = 0\n",
    "    value_change = 1\n",
    "\n",
    "    list1 = [j]\n",
    "    list2 = [j_val]\n",
    "\n",
    "    while count <= 1000 and value_change >= 2**-23: # number of epoch\n",
    "        #print(count)\n",
    "        omega  = omega + (10**(-4))*(X_train.T.dot(y_train-y_hat))\n",
    "        y_hat = 1 / (1 + np.exp(-X_train.dot(omega)))\n",
    "        for i in range(len(y_hat)):\n",
    "            if y_hat[i][0] == 1:\n",
    "                y_hat[i][0] = y_hat[i][0] - 0.01\n",
    "            if y_hat[i][0] == 0:\n",
    "                y_hat[i][0] = y_hat[i][0] + 0.01\n",
    "        j_new = np.mean(y_train*(np.log(y_hat))+(1-y_train)*(np.log(1-y_hat)))\n",
    "        value_change = np.abs(j_new-j)\n",
    "\n",
    "        y_hat_val = 1 / (1 + np.exp(-X_val.dot(omega)))\n",
    "        for k in range(len(y_hat_val)):\n",
    "            if y_hat_val[k][0] == 1:\n",
    "                y_hat_val[k][0] = y_hat_val[k][0] - 0.01\n",
    "            if y_hat_val[k][0] == 0:\n",
    "                y_hat_val[k][0] = y_hat_val[k][0] + 0.01\n",
    "        j_val_new = np.mean(y_val*(np.log(y_hat_val))+(1-y_val)*(np.log(1-y_hat_val)))\n",
    "\n",
    "        list1.append(j_new)\n",
    "        list2.append(j_val_new)\n",
    "        j = j_new\n",
    "        count += 1\n",
    "\n",
    "    pre_train, recall_train, f_train, accuracy_train = cal_pr(0.5,y_hat, y_train )\n",
    "    print(\"Training\")\n",
    "    print(\"Precision\\tRecall\\t\\tf-Measure\\tAccuracy\")\n",
    "    print(\"{:.3f}\\t\\t{:.3f}\\t\\t{:.3f}\\t\\t{:.3f}\".format(pre_train, recall_train, f_train, accuracy_train))\n",
    "\n",
    "    pre_valid, recall_valid, f_valid, accuracy_valid = cal_pr(0.5,y_hat_val, y_val )\n",
    "    print(\"Validation\")\n",
    "    print(\"Precision\\tRecall\\t\\tf-Measure\\tAccuracy\")\n",
    "    print(\"{:.3f}\\t\\t{:.3f}\\t\\t{:.3f}\\t\\t{:.3f}\".format(pre_valid, recall_valid, f_valid, accuracy_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Precision\tRecall\t\tf-Measure\tAccuracy\n",
      "0.976\t\t1.000\t\t0.988\t\t0.976\n",
      "Validation\n",
      "Precision\tRecall\t\tf-Measure\tAccuracy\n",
      "0.977\t\t1.000\t\t0.988\t\t0.977\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression for 18-Features dataset\n",
    "Logistic_Regression('18features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Precision\tRecall\t\tf-Measure\tAccuracy\n",
      "0.976\t\t1.000\t\t0.988\t\t0.976\n",
      "Validation\n",
      "Precision\tRecall\t\tf-Measure\tAccuracy\n",
      "0.977\t\t1.000\t\t0.988\t\t0.977\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes for 12-paper-Features dataset\n",
    "Logistic_Regression('12paper_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Precision\tRecall\t\tf-Measure\tAccuracy\n",
      "0.976\t\t1.000\t\t0.988\t\t0.976\n",
      "Validation\n",
      "Precision\tRecall\t\tf-Measure\tAccuracy\n",
      "0.977\t\t1.000\t\t0.988\t\t0.977\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression for 12-top-Features dataset\n",
    "Logistic_Regression('12top_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Precision\tRecall\t\tf-Measure\tAccuracy\n",
      "0.692\t\t0.594\t\t0.639\t\t0.683\n",
      "Validation\n",
      "Precision\tRecall\t\tf-Measure\tAccuracy\n",
      "0.697\t\t0.600\t\t0.645\t\t0.635\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression for 12-paper-Features balanced dataset\n",
    "Logistic_Regression('12paper_features_balanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Precision\tRecall\t\tf-Measure\tAccuracy\n",
      "0.709\t\t0.619\t\t0.661\t\t0.700\n",
      "Validation\n",
      "Precision\tRecall\t\tf-Measure\tAccuracy\n",
      "0.683\t\t0.487\t\t0.569\t\t0.591\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression for 12-top-Features balanced dataset\n",
    "Logistic_Regression('12top_features_balanced.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to z_score the training dataset \n",
    "def zscore_train(dataset):\n",
    "    zx_train = (dataset - dataset.mean(axis = 0))/np.std(dataset,axis = 0,ddof = 1)\n",
    "    return zx_train\n",
    "# Function to z-score the validation dataset using the mean and standard deviation of the coressponding training dataset\n",
    "def zscore_valid(valid_set, train_set):\n",
    "    zx_valid = (valid_set - train_set.mean(axis = 0))/np.std(train_set,axis = 0, ddof =1)\n",
    "    return zx_valid\n",
    "\n",
    "# Function to count for output true positive (TP), true negative (TN), false positive (FP), false negative (FN)\n",
    "def confMatrixPara(pred_y, y):\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if pred_y[i] == y[i] ==1:\n",
    "            TP +=1\n",
    "        elif pred_y[i] == y[i] ==0:\n",
    "            TN +=1\n",
    "        elif (pred_y[i] == 0) and (y[i] ==1):\n",
    "            FN +=1\n",
    "        else:\n",
    "            FP +=1\n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "def count_class(array_y):\n",
    "    ls = []\n",
    "    for i in array_y:\n",
    "        if i not in ls:\n",
    "            ls.append(i)\n",
    "    return ls\n",
    "        \n",
    "    \n",
    "# Function to calculate for the required statistics\n",
    "def statistics_result(TP, TN, FP, FN, array):\n",
    "    precision =TP/(TP+FP)\n",
    "    recall = TP/(TP +FN) \n",
    "    f_measure = 2*precision*recall/(precision+recall)\n",
    "    accuracy = (1/len(array))*(TP+TN)\n",
    "    return precision, recall, f_measure, accuracy\n",
    "\n",
    "def pdf(x, mean, std):\n",
    "    return np.exp(-0.5*((x-mean)/std)**2)/(std*np.sqrt(2*np.pi))\n",
    "\n",
    "def probability(x_valid, prior, mean_ls, std_ls):\n",
    "    prob = np.log(prior)\n",
    "    for i in range(len(mean_ls)):\n",
    "        pdf_i = pdf(x_valid[i],mean_ls[i], std_ls[i])\n",
    "        if pdf_i ==0:\n",
    "            return 0\n",
    "        else:\n",
    "            prob = prob+ np.log(pdf_i)\n",
    "    return np.exp(prob)\n",
    "\n",
    "# Function to apply threshold for classification\n",
    "def apply_threshold(array, threshold):\n",
    "    array2 = np.zeros((array.shape))\n",
    "    for i in range(len(array)):\n",
    "        if array[i] >= threshold:\n",
    "            array2[i] = 1\n",
    "        else:\n",
    "            continue\n",
    "    return array2\n",
    "def Naive_Bayes(filename):\n",
    "    data = pd.read_csv(filename, sep = ',')\n",
    "    data_list = data.values.tolist()\n",
    "\n",
    "    # Randomize the data\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(data_list)\n",
    "    data_rand = np.array(data_list)\n",
    "\n",
    "\n",
    "    # Separate features data and rename to x and y\n",
    "    y_data = np.array(data_rand[:, -1])\n",
    "    x_data = np.array(data_rand[:, :-1])\n",
    "\n",
    "    # Selecte the first 2/3 (round up) of the data for training and the remaining for validation\n",
    "    num_train = math.ceil(((2/3)*len(data_rand)))\n",
    "    y_train = y_data[:num_train]\n",
    "    x_train = x_data[:num_train,:]\n",
    "    y_valid = y_data[num_train:]\n",
    "    x_valid = x_data[num_train:,:]\n",
    "\n",
    "    # Zcores the features training data\n",
    "    zx_train = zscore_train(x_train)\n",
    "    zx_valid = zscore_valid(x_valid, x_train)\n",
    "\n",
    "    class_set = count_class(y_train)\n",
    "    data_set = []\n",
    "    prior_set = []\n",
    "    mean_set= []\n",
    "    std_set = []\n",
    "    for i in class_set:\n",
    "        array = zx_train[y_train==i]\n",
    "        data_set.append(array)\n",
    "        prior_= len(array)/len(zx_train)\n",
    "        prior_set.append(prior_)\n",
    "        mean_ =np.array([np.mean(array[:, j]) for j in range(array.shape[1])])\n",
    "        std_ = np.array([np.std(array[:,j],ddof = 1) for j in range(array.shape[1])])\n",
    "        mean_ = mean_[std_ >= 1e-4]\n",
    "        std_ = std_[std_ >= 1e-4]\n",
    "        mean_set.append(mean_)\n",
    "        std_set.append(std_)\n",
    "\n",
    "    training =[]\n",
    "    for i in range(len(zx_train)):\n",
    "        p_set = []\n",
    "        for j in range(len(class_set)):\n",
    "            p_ = probability(zx_train[i], prior_set[j], mean_set[j], std_set[j])\n",
    "            p_set.append(p_)\n",
    "        pred_train_y =class_set[p_set.index(max(p_set))]\n",
    "        training.append(pred_train_y)\n",
    "\n",
    "    TP_train, TN_train, FP_train, FN_train = confMatrixPara(training, y_train)\n",
    "    pre_train, recall_train, f_train, accuracy_train = statistics_result(TP_train, TN_train, FP_train, FN_train, y_train)\n",
    "    print(\"Training\")\n",
    "    print(\"Precision\\tRecall\\t\\tf-Measure\\tAccuracy\")\n",
    "    print(\"{:.3f}\\t\\t{:.3f}\\t\\t{:.3f}\\t\\t{:.3f}\".format(pre_train, recall_train, f_train, accuracy_train))\n",
    "\n",
    "    prediction =[]\n",
    "    for i in range(len(zx_valid)):\n",
    "        p_set = []\n",
    "        for j in range(len(class_set)):\n",
    "            p_ = probability(zx_valid[i], prior_set[j], mean_set[j], std_set[j])\n",
    "            p_set.append(p_)\n",
    "        pred_y =class_set[p_set.index(max(p_set))]\n",
    "        prediction.append(pred_y)\n",
    "\n",
    "    TP_valid, TN_valid, FP_valid, FN_valid = confMatrixPara(prediction, y_valid)\n",
    "    pre_valid, recall_valid, f_valid, accuracy_valid = statistics_result(TP_valid, TN_valid, FP_valid, FN_valid, y_valid)\n",
    "    print(\"Validation\")\n",
    "    print(\"Precision\\tRecall\\t\\tf-Measure\\tAccuracy\")\n",
    "    print(\"{:.3f}\\t\\t{:.3f}\\t\\t{:.3f}\\t\\t{:.3f}\".format(pre_valid, recall_valid, f_valid, accuracy_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Precision\tRecall\t\tf-Measure\tAccuracy\n",
      "0.980\t\t0.943\t\t0.961\t\t0.925\n",
      "Validation\n",
      "Precision\tRecall\t\tf-Measure\tAccuracy\n",
      "0.980\t\t0.940\t\t0.960\t\t0.923\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes for 18-Features dataset\n",
    "Naive_Bayes('18features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Precision\tRecall\t\tf-Measure\tAccuracy\n",
      "0.979\t\t0.963\t\t0.971\t\t0.943\n",
      "Validation\n",
      "Precision\tRecall\t\tf-Measure\tAccuracy\n",
      "0.979\t\t0.960\t\t0.969\t\t0.941\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes for 12-paper-features dataset\n",
    "Naive_Bayes('12paper_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Precision\tRecall\t\tf-Measure\tAccuracy\n",
      "0.979\t\t0.951\t\t0.965\t\t0.932\n",
      "Validation\n",
      "Precision\tRecall\t\tf-Measure\tAccuracy\n",
      "0.980\t\t0.946\t\t0.962\t\t0.928\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes for 12-top-features dataset\n",
    "Naive_Bayes('12top_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Precision\tRecall\t\tf-Measure\tAccuracy\n",
      "0.722\t\t0.528\t\t0.610\t\t0.680\n",
      "Validation\n",
      "Precision\tRecall\t\tf-Measure\tAccuracy\n",
      "0.730\t\t0.470\t\t0.571\t\t0.611\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes for 12-top-features dataset\n",
    "Naive_Bayes('12paper_features_balanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Precision\tRecall\t\tf-Measure\tAccuracy\n",
      "0.727\t\t0.594\t\t0.654\t\t0.702\n",
      "Validation\n",
      "Precision\tRecall\t\tf-Measure\tAccuracy\n",
      "0.675\t\t0.487\t\t0.566\t\t0.587\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes for 12-top-features dataset\n",
    "Naive_Bayes('12top_features_balanced.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function return the feature that has min entropy\n",
    "def find_node(dataset):\n",
    "    en_all = []\n",
    "    for i in range(dataset.shape[1]-1):\n",
    "        en_ls = []\n",
    "        count_ls = []\n",
    "        d_br = count_branch(dataset[:,i],dataset[:,-1])\n",
    "        for key in d_br.keys():\n",
    "            ind_en, ind_count = entropy_br(d_br[key])\n",
    "            en_ls.append(ind_en)\n",
    "            count_ls.append(ind_count)\n",
    "        en_avg = entropy_var(np.array(en_ls), np.array(count_ls))\n",
    "        en_all.append(en_avg)\n",
    "    node = en_all.index(min(en_all))\n",
    "    return node\n",
    "\n",
    "# array is a dictionaty\n",
    "def leaf_node(array):\n",
    "    d = dict()\n",
    "    for key in array.keys():\n",
    "        num = 0\n",
    "        for item in array[key]:\n",
    "            if num <= array[key][item]:\n",
    "                num = array[key][item]\n",
    "                d[key]=item\n",
    "    return d\n",
    "\n",
    "\n",
    "def create_set(pre_set, node):\n",
    "    count_br = set(pre_set[:,node])\n",
    "    d = {}\n",
    "    data = np.delete(pre_set, obj = node,axis =1)\n",
    "    for i in range(len(count_br)):\n",
    "        array_ls = []\n",
    "        for j in range(len(pre_set)):\n",
    "            if pre_set[:,node][j]==(list(count_br))[i]:\n",
    "                array_ls.append(data[j,:])\n",
    "        d[(list(count_br))[i]] = np.array(array_ls)\n",
    "\n",
    "    return d\n",
    "\n",
    "def DTL(data,root, d):\n",
    "    if len(data)<=1:\n",
    "        children=[]\n",
    "        for key, array in data.items():\n",
    "            n = array\n",
    "            shape_n = (np.array(n)).shape\n",
    "            ls_y =[]\n",
    "            for i in array[:,-1]:\n",
    "                if i not in ls_y:\n",
    "                    ls_y.append(i)\n",
    "            if shape_n[0]<=1:\n",
    "                return n[0][-1]\n",
    "            elif shape_n[1] <=2:\n",
    "                if len(ls_y)<=1:\n",
    "                    return ls_y[0]\n",
    "\n",
    "                num_br0 = count_branch(array[:,:-1],array[:,-1])\n",
    "                if len(num_br0)<=1:\n",
    "                    dict_ = list(num_br0.values())\n",
    "                    keys_ = list(dict_[0].keys())\n",
    "                    values_ = list(dict_[0].values())\n",
    "                    a = keys_[values_.index(max(values_))]\n",
    "                    return a\n",
    "                else:\n",
    "                    d = leaf_node(num_br0)\n",
    "                    return d\n",
    "            elif len(ls_y)<=1:\n",
    "                return ls_y[0]\n",
    "            else:\n",
    "                node = find_node(array)\n",
    "                child_data = create_set(array,node)\n",
    "                children.append({key:DTL(child_data,node,d)})\n",
    "    else:\n",
    "        children = []\n",
    "        for value, array in data.items():\n",
    "            if array.ndim ==1:\n",
    "                continue\n",
    "            elif array.shape[1] <=2:\n",
    "                num_br0 = count_branch(array[:,:-1],array[:,-1])\n",
    "                if np.isscalar(num_br0):\n",
    "                    d = num_br0\n",
    "                else:\n",
    "                    d = leaf_node(num_br0)\n",
    "                return d\n",
    "            else:\n",
    "                node = find_node(array)\n",
    "                child_data = create_set(array,node)\n",
    "                children.append({value:DTL(child_data,node,d)})\n",
    "        \n",
    "        return {root:children}\n",
    "    \n",
    "def testing_tree(valid_sample, tree):\n",
    "    if tree is None:\n",
    "        return 1\n",
    "    elif len(tree)>0:\n",
    "        node = list(tree.keys())\n",
    "        values = list(tree.values()) #list dict type\n",
    "        if np.isscalar(values[0]):\n",
    "            return values\n",
    "        else:\n",
    "            ans = valid_sample[node]\n",
    "            for i in range(len(values[0])):\n",
    "                d = values[0][i]\n",
    "                if ans != list(d.keys()):\n",
    "                    continue\n",
    "                else:\n",
    "                    new_tree = list(d.values())[0]\n",
    "                    if np.isscalar(new_tree):\n",
    "                        return new_tree\n",
    "                    new_array = del_col(valid_sample, node)\n",
    "                    result = testing_tree(new_array, new_tree)\n",
    "                return result\n",
    "\n",
    "    else:\n",
    "        d = list(tree.values())\n",
    "        return d\n",
    "\n",
    "def del_col(valid_sample, node):\n",
    "    new_array = np.delete(valid_sample, obj = node)\n",
    "    return new_array\n",
    "\n",
    "def decision_tree(filename):\n",
    "    # Read in the data from csv file and save it to a list\n",
    "    data = pd.read_csv(filename, sep = ',')\n",
    "    data_list = data.values.tolist()\n",
    "\n",
    "    # Randomize the data\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(data_list)\n",
    "    data_rand = np.array(data_list)\n",
    "\n",
    "    # Separate features data and rename to x and y\n",
    "    y_data = np.array(data_rand[:, -1])\n",
    "    x_data = np.array(data_rand[:, :-1])\n",
    "\n",
    "    # Selecte the first 2/3 (round up) of the data for training and the remaining for validation\n",
    "    num_train = math.ceil(((2/3)*len(data_rand)))\n",
    "    y_train = y_data[:num_train]\n",
    "    x_train = x_data[:num_train,:]\n",
    "    y_valid = y_data[num_train:]\n",
    "    x_valid = x_data[num_train:,:]\n",
    "\n",
    "    # Zcores the features training data\n",
    "    zx_train = zscore_train(x_train)\n",
    "    zx_valid = zscore_valid(x_valid, x_train)\n",
    "\n",
    "    mean_train =np.array([np.mean(zx_train[:, i]) for i in range(zx_train.shape[1])])\n",
    "    mean_valid =np.array([np.mean(zx_valid[:, i]) for i in range(zx_train.shape[1])])\n",
    "\n",
    "    zx_trainb = np.zeros((zx_train.shape))\n",
    "    for i in range(zx_train.shape[1]):\n",
    "        for j in range(len(zx_train)):\n",
    "            if zx_train[j, i] >= mean_train[i]:\n",
    "                zx_trainb[j,i] = 1\n",
    "            else:\n",
    "                continue\n",
    "    zx_validb = np.zeros((zx_valid.shape))\n",
    "    for i in range(zx_valid.shape[1]):\n",
    "        for j in range(len(zx_valid)):\n",
    "            if zx_valid[j, i] >= mean_valid[i]:\n",
    "                zx_validb[j,i] = 1\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    z_train = np.concatenate((zx_trainb, np.reshape(y_train, (len(y_train), 1))), axis =1)\n",
    "\n",
    "    data = z_train[:, :]\n",
    "    tree = defaultdict()\n",
    "    root = find_node(data)\n",
    "    dict_ = create_set(data, root)\n",
    "\n",
    "    decision_tree = DTL(dict_,root, None)\n",
    "    training =[]\n",
    "    for i in range(len(zx_trainb)):\n",
    "        pre = testing_tree(zx_trainb[i], decision_tree)\n",
    "        training.append(pre)\n",
    "\n",
    "    TP_train, TN_train, FP_train, FN_train = confMatrixPara(training, y_train)\n",
    "    pre_train, recall_train, f_train, accuracy_train = statistics_result(TP_train, TN_train, FP_train, FN_train, y_train)\n",
    "    print(\"Training\")\n",
    "    print(\"Precision\\tRecall\\t\\tf-Measure\\tAccuracy\")\n",
    "    print(\"{:.3f}\\t\\t{:.3f}\\t\\t{:.3f}\\t\\t{:.3f}\".format(pre_train, recall_train, f_train, accuracy_train))\n",
    "\n",
    "    pre_y = []\n",
    "    for i in range(len(zx_validb)):\n",
    "        pre = testing_tree(zx_validb[i], decision_tree)\n",
    "        pre_y.append(pre)\n",
    "\n",
    "    TP_valid, TN_valid, FP_valid, FN_valid = confMatrixPara(pre_y, y_valid)\n",
    "    pre_valid, recall_valid, f_valid, accuracy_valid = statistics_result(TP_valid, TN_valid, FP_valid, FN_valid, y_valid)\n",
    "    print(\"Validation\")\n",
    "    print(\"Precision\\tRecall\\t\\tf-Measure\\tAccuracy\")\n",
    "    print(\"{:.3f}\\t\\t{:.3f}\\t\\t{:.3f}\\t\\t{:.3f}\".format(pre_valid, recall_valid, f_valid, accuracy_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Precision\tRecall\t\tf-Measure\tAccuracy\n",
      "0.985\t\t1.000\t\t0.992\t\t0.985\n",
      "Validation\n",
      "Precision\tRecall\t\tf-Measure\tAccuracy\n",
      "0.977\t\t0.985\t\t0.981\t\t0.963\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree for 18-Features dataset\n",
    "decision_tree('18features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Precision\tRecall\t\tf-Measure\tAccuracy\n",
      "0.976\t\t1.000\t\t0.988\t\t0.975\n",
      "Validation\n",
      "Precision\tRecall\t\tf-Measure\tAccuracy\n",
      "0.975\t\t0.999\t\t0.987\t\t0.973\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree for 12-paper-Features dataset\n",
    "decision_tree('12paper_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Precision\tRecall\t\tf-Measure\tAccuracy\n",
      "0.977\t\t1.000\t\t0.988\t\t0.976\n",
      "Validation\n",
      "Precision\tRecall\t\tf-Measure\tAccuracy\n",
      "0.976\t\t0.997\t\t0.986\t\t0.973\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree for 12top-Features dataset\n",
    "decision_tree('12top_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Precision\tRecall\t\tf-Measure\tAccuracy\n",
      "0.716\t\t0.906\t\t0.800\t\t0.815\n",
      "Validation\n",
      "Precision\tRecall\t\tf-Measure\tAccuracy\n",
      "0.504\t\t0.652\t\t0.569\t\t0.562\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree for 12-paper-Features balanced dataset\n",
    "decision_tree('12paper_features_balanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Precision\tRecall\t\tf-Measure\tAccuracy\n",
      "0.702\t\t0.952\t\t0.808\t\t0.817\n",
      "Validation\n",
      "Precision\tRecall\t\tf-Measure\tAccuracy\n",
      "0.540\t\t0.698\t\t0.609\t\t0.587\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree for 12-top-Features balanced dataset\n",
    "decision_tree('12top_features_balanced.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(train, valid, sample_size, tree_numbers):\n",
    "    y_valid = valid[:,-1]\n",
    "    x_valid = valid[:,:-1]\n",
    "\n",
    "    # sample size\n",
    "    count = 0\n",
    "    tree_ls = []\n",
    "    while count < tree_numbers:\n",
    "        train_sample = np.array(random.choices(train,k =sample_size))\n",
    "        y_train = train_sample[:,-1]\n",
    "        x_train = train_sample[:,:-1]\n",
    "\n",
    "        # Zcores the features training data\n",
    "        zx_train = zscore_train(x_train)\n",
    "        zx_valid = zscore_valid(x_valid, x_train)\n",
    "\n",
    "        mean_train =np.array([np.mean(zx_train[:, i]) for i in range(zx_train.shape[1])])\n",
    "        mean_valid =np.array([np.mean(zx_valid[:, i]) for i in range(zx_train.shape[1])])\n",
    "\n",
    "        zx_trainb = np.zeros((zx_train.shape))\n",
    "        for i in range(zx_train.shape[1]):\n",
    "            for j in range(len(zx_train)):\n",
    "                if zx_train[j, i] >= mean_train[i]:\n",
    "                    zx_trainb[j,i] = 1\n",
    "                else:\n",
    "                    continue\n",
    "        zx_validb = np.zeros((zx_valid.shape))\n",
    "        for i in range(zx_valid.shape[1]):\n",
    "            for j in range(len(zx_valid)):\n",
    "                if zx_valid[j, i] >= mean_valid[i]:\n",
    "                    zx_validb[j,i] = 1\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "        z_train = np.concatenate((zx_trainb, np.reshape(y_train, (len(y_train), 1))), axis =1)\n",
    "\n",
    "        data = z_train[:, :]\n",
    "\n",
    "        root = find_node(data) \n",
    "        dict_ = create_set(data, root)\n",
    "        decision_tree = DTL(dict_,root, None)\n",
    "        #print(decision_tree)\n",
    "        tree_ls.append(decision_tree)\n",
    "        count+=1\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(len(zx_validb)):\n",
    "        pre_ls = []\n",
    "        for j in range(len(tree_ls)):\n",
    "            pre = testing_tree(zx_validb[i], tree_ls[j])\n",
    "            pre_ls.append(pre)\n",
    "        for item in range(len(pre_ls)):\n",
    "            if isinstance(pre_ls[item], list):\n",
    "                ls = pre_ls[item]\n",
    "                pre_ls[item]=ls[0]\n",
    "        pre_y = max(set(pre_ls), key = pre_ls.count)\n",
    "        predictions.append(pre_y)\n",
    "\n",
    "    TP_valid, TN_valid, FP_valid, FN_valid = confMatrixPara(predictions, y_valid)\n",
    "    pre_valid, recall_valid, f_valid, accuracy_valid = statistics_result(TP_valid, TN_valid, FP_valid, FN_valid, y_valid)\n",
    "    \n",
    "    return pre_valid, recall_valid, f_valid, accuracy_valid\n",
    "\n",
    "def random_forest_results(filename, tree_numbers, sample_size):\n",
    "    # Read in the data from csv file and save it to a list\n",
    "    df = pd.read_csv(filename, sep = ',')\n",
    "    data_list = df.values.tolist()\n",
    "\n",
    "    # Randomize the data\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(data_list)\n",
    "\n",
    "\n",
    "    # Selecte the first 2/3 (round up) of the data for training and the remaining for validation\n",
    "    num_train = int(np.ceil((2/3)*len(data_list)))\n",
    "    train = data_list[:num_train]\n",
    "    valid = np.array(data_list[num_train:])\n",
    "\n",
    "    print(\"#tree\\tPrecision\\tRecall\\t\\tf-Measure\\tAccuracy\")\n",
    "    for i in tree_numbers:\n",
    "        pre_valid, recall_valid, f_valid, accuracy_valid = random_forest(train, valid, sample_size, i)\n",
    "        print(\"{:.2f}\\t{:.3f}\\t\\t{:.3f}\\t\\t{:.3f}\\t\\t{:.3f}\".format(i, pre_valid, recall_valid, f_valid, accuracy_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tree\tPrecision\tRecall\t\tf-Measure\tAccuracy\n",
      "1.00\t0.978\t\t0.988\t\t0.983\t\t0.967\n",
      "10.00\t0.977\t\t0.994\t\t0.986\t\t0.972\n"
     ]
    }
   ],
   "source": [
    "# Random Forest for 18-Features dataset with sample size = 8000\n",
    "random_forest_results('18features.csv', [1,10],8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tree\tPrecision\tRecall\t\tf-Measure\tAccuracy\n",
      "1.00\t0.977\t\t0.991\t\t0.984\t\t0.968\n",
      "10.00\t0.977\t\t0.998\t\t0.987\t\t0.975\n",
      "100.00\t0.977\t\t0.999\t\t0.988\t\t0.976\n",
      "200.00\t0.977\t\t1.000\t\t0.988\t\t0.977\n",
      "300.00\t0.977\t\t0.999\t\t0.988\t\t0.976\n",
      "500.00\t0.977\t\t0.999\t\t0.988\t\t0.976\n"
     ]
    }
   ],
   "source": [
    "# Random Forest for 12-paper-Features dataset with sample size = 8000\n",
    "random_forest_results('12paper_features.csv', [1,10,100,200,300,500],8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tree\tPrecision\tRecall\t\tf-Measure\tAccuracy\n",
      "1.00\t0.977\t\t0.988\t\t0.983\t\t0.966\n",
      "10.00\t0.977\t\t0.998\t\t0.987\t\t0.975\n"
     ]
    }
   ],
   "source": [
    "# Random Forest for 12-paper-Features dataset with sample size = 6000\n",
    "random_forest_results('12paper_features.csv', [1,10],6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tree\tPrecision\tRecall\t\tf-Measure\tAccuracy\n",
      "1.00\t0.977\t\t0.993\t\t0.985\t\t0.970\n",
      "10.00\t0.977\t\t0.999\t\t0.988\t\t0.975\n"
     ]
    }
   ],
   "source": [
    "# Random Forest for 12-top-features dataset with sample size = 8000\n",
    "random_forest_results('12top_features.csv', [1,10],8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 Cross Validation on Balanced Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_valid_LR(filename, k):\n",
    "    sample = pd.read_csv(filename,header=0)\n",
    "    sample.shape\n",
    "    from scipy import stats\n",
    "    from sklearn.model_selection import KFold\n",
    "    for item in k:\n",
    "        kf = KFold(n_splits=item)\n",
    "        trains = []\n",
    "        tests = []\n",
    "        for train,test in kf.split(sample):\n",
    "            trains.append(train)\n",
    "            tests.append(test)\n",
    "\n",
    "        mse = []\n",
    "        ls = []\n",
    "        for num in range(item):\n",
    "            cv_train = sample.iloc[trains[num]]\n",
    "            cv_test = sample.iloc[tests[num]]\n",
    "            train_X = np.ones(cv_train.shape[0]).reshape(-1,1)\n",
    "            test_X = np.ones(cv_test.shape[0]).reshape(-1,1)\n",
    "            train_y = cv_train.iloc[:,12].to_numpy().reshape(-1,1)\n",
    "            test_y = cv_test.iloc[:,12].to_numpy().reshape(-1,1)\n",
    "\n",
    "            for i in range (12):\n",
    "                cv_train_temp = cv_train.iloc[:,i].to_numpy()\n",
    "                cv_train_temp_mean = cv_train.iloc[:,i].mean()\n",
    "                cv_train_temp_std = cv_train.iloc[:,i].std()\n",
    "                cv_train_temp_stand= (cv_train_temp-cv_train_temp_mean)/cv_train_temp_std\n",
    "\n",
    "                train_X = np.append(train_X,cv_train_temp_stand.reshape(-1,1),axis=1) \n",
    "\n",
    "                cv_test_temp = cv_test.iloc[:,i].to_numpy()\n",
    "                cv_test_temp_stand = (cv_test_temp-cv_train_temp_mean)/cv_train_temp_std\n",
    "\n",
    "                test_X = np.append(test_X,cv_test_temp_stand.reshape(-1,1),axis=1)\n",
    "\n",
    "            omega = np.random.uniform(-0.01,0.01,train_X.shape[1]).reshape(-1,1)\n",
    "\n",
    "            X_train = train_X\n",
    "            y_hat = 1 / (1 + np.exp(-X_train.dot(omega)))\n",
    "            y_train = train_y\n",
    "            j = np.mean(y_train*(np.log(y_hat))+(1-y_train)*(np.log(1-y_hat)))\n",
    "            y_val = test_y\n",
    "            X_val = test_X\n",
    "            y_hat_val = 1 / (1 + np.exp(-X_val.dot(omega)))\n",
    "            j_val = np.mean(y_val*(np.log(y_hat_val))+(1-y_val)*(np.log(1-y_hat_val)))\n",
    "\n",
    "            # terminate until meet criteria\n",
    "            count = 0\n",
    "            value_change = 1\n",
    "\n",
    "            list1 = [j]\n",
    "            list2 = [j_val]\n",
    "\n",
    "            while count <= 1000 and value_change >= 2**-23:\n",
    "                #print(count)\n",
    "                omega  = omega + (10**(-4))*(X_train.T.dot(y_train-y_hat))\n",
    "                y_hat = 1 / (1 + np.exp(-X_train.dot(omega)))\n",
    "                for i in range(len(y_hat)):\n",
    "                    if y_hat[i][0] == 1:\n",
    "                        y_hat[i][0] = y_hat[i][0] - 0.01\n",
    "                    if y_hat[i][0] == 0:\n",
    "                        y_hat[i][0] = y_hat[i][0] + 0.01\n",
    "                j_new = np.mean(y_train*(np.log(y_hat))+(1-y_train)*(np.log(1-y_hat)))\n",
    "                value_change = np.abs(j_new-j)\n",
    "\n",
    "                y_hat_val = 1 / (1 + np.exp(-X_val.dot(omega)))\n",
    "                for k in range(len(y_hat_val)):\n",
    "                    if y_hat_val[k][0] == 1:\n",
    "                        y_hat_val[k][0] = y_hat_val[k][0] - 0.01\n",
    "                    if y_hat_val[k][0] == 0:\n",
    "                        y_hat_val[k][0] = y_hat_val[k][0] + 0.01\n",
    "                j_val_new = np.mean(y_val*(np.log(y_hat_val))+(1-y_val)*(np.log(1-y_hat_val)))\n",
    "\n",
    "                list1.append(j_new)\n",
    "                list2.append(j_val_new)\n",
    "                j = j_new\n",
    "                count +=1\n",
    "        n = 0\n",
    "        #y_hat_val = 1 / (1 + np.exp(-X_val.dot(omega)))\n",
    "        for i in range(len(y_val)):\n",
    "            if y_hat_val[i][0] >= 0.5 and y_val[i][0] == 1:\n",
    "                n += 1\n",
    "            elif y_hat_val[i][0] < 0.5 and y_val[i][0] == 0:\n",
    "                n += 1\n",
    "\n",
    "        acc = n/len(y_val)\n",
    "        ls.append(acc)\n",
    "        a = sum(ls)/len(ls)\n",
    "        print(\"k\\tAccuracy\")\n",
    "        print(\"{:.2f}\\t{:.3f}\".format(item, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k\tAccuracy\n",
      "\n",
      "5.00\t0.484\n",
      "k\tAccuracy\n",
      "\n",
      "10.00\t0.645\n",
      "k\tAccuracy\n",
      "\n",
      "50.00\t0.667\n",
      "k\tAccuracy\n",
      "\n",
      "100.00\t0.667\n",
      "k\tAccuracy\n",
      "\n",
      "500.00\t1.000\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation Logistic Regression on 12-paper-features-balanced dataset\n",
    "cross_valid_LR(\"12paper_features_balanced.csv\", [5,10,50,100,500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_valid_NB(filename, k_ls):\n",
    "    from sklearn.model_selection import KFold\n",
    "    sample = pd.read_csv(filename,header=0)\n",
    "    sample.shape\n",
    "    \n",
    "    for item in k_ls:\n",
    "        kf = KFold(n_splits=item)\n",
    "        trains = []\n",
    "        tests = []\n",
    "        for train,test in kf.split(sample):\n",
    "            trains.append(train)\n",
    "            tests.append(test)\n",
    "\n",
    "        mse = []\n",
    "        ls = []\n",
    "        for num in range(item):\n",
    "            cv_train = sample.iloc[trains[num]]\n",
    "            cv_test = sample.iloc[tests[num]]\n",
    "            train_X = np.ones(cv_train.shape[0]).reshape(-1,1)\n",
    "            test_X = np.ones(cv_test.shape[0]).reshape(-1,1)\n",
    "            train_y = cv_train.iloc[:,12].to_numpy().reshape(-1,1)\n",
    "            test_y = cv_test.iloc[:,12].to_numpy().reshape(-1,1)\n",
    "\n",
    "            for i in range (12):\n",
    "\n",
    "                cv_train_temp = cv_train.iloc[:,i].to_numpy()\n",
    "                cv_train_temp_mean = cv_train.iloc[:,i].mean()\n",
    "                cv_train_temp_std = cv_train.iloc[:,i].std()\n",
    "                cv_train_temp_stand= (cv_train_temp-cv_train_temp_mean)/cv_train_temp_std\n",
    "\n",
    "                train_X = np.append(train_X,cv_train_temp_stand.reshape(-1,1),axis=1) \n",
    "\n",
    "                cv_test_temp = cv_test.iloc[:,i].to_numpy()\n",
    "                cv_test_temp_stand = (cv_test_temp-cv_train_temp_mean)/cv_train_temp_std\n",
    "\n",
    "                test_X = np.append(test_X,cv_test_temp_stand.reshape(-1,1),axis=1)\n",
    "\n",
    "                X_and_lastcol = np.append(train_X,train_y.reshape(-1,1),axis=1)\n",
    "                X_df = pd.DataFrame(X_and_lastcol)\n",
    "\n",
    "\n",
    "            train_spam = X_df[X_df[13]==1]\n",
    "\n",
    "            # split into Non-Spam samples\n",
    "            train_nonspam = X_df[X_df[13]==0]\n",
    "\n",
    "            dict_spam = {}\n",
    "            for i in range(13):\n",
    "                arr = train_spam.iloc[:,i].to_numpy()\n",
    "                dict_spam[i] = []\n",
    "                dict_spam[i].append(np.mean(arr))\n",
    "                dict_spam[i].append(np.var(arr,ddof=1))\n",
    "                dict_spam[i].append(np.std(arr,ddof=1))\n",
    "\n",
    "\n",
    "            # create a dictionary to store the mean,variance,std for each feature in Non-Spam class\n",
    "            dict_nonspam = {}\n",
    "            for i in range(13):\n",
    "                arr = train_nonspam.iloc[:,i].to_numpy()\n",
    "                dict_nonspam[i] = []\n",
    "                dict_nonspam[i].append(np.mean(arr))\n",
    "                dict_nonspam[i].append(np.var(arr,ddof=1))\n",
    "                dict_nonspam[i].append(np.std(arr,ddof=1))\n",
    "\n",
    "            import math\n",
    "            # calculate p(y=Spam)\n",
    "            pro_spam = len(train_spam)/len(train_X)\n",
    "\n",
    "            # calculate p(y=Non-Spam)\n",
    "            pro_nonspam = len(train_nonspam)/len(train_X)\n",
    "\n",
    "            # create a function to calculate guassian distribution\n",
    "            def P_spam(x, mean, std):\n",
    "                var = float(std)**2+10**-100\n",
    "                denom = (2*math.pi*var)**.5\n",
    "                num = math.exp(-(float(x)-float(mean))**2/(2*var))\n",
    "                return num/denom\n",
    "\n",
    "\n",
    "            # calcualte all the probability in spam class\n",
    "            lst_spam = []\n",
    "            for row in range(len(test_X)):\n",
    "                lst_s = []\n",
    "                for col in range(12):\n",
    "                    pro = P_spam(test_X[row,:][col],dict_spam[col][0],dict_spam[col][2])\n",
    "                    lst_s.append(pro+10**-100)\n",
    "                a= np.array(lst_s)\n",
    "                product = np.log(pro_spam) + np.sum(np.log(a))\n",
    "                lst_spam.append(product)\n",
    "\n",
    "            # calcualte all the probability in non-spam class\n",
    "            lst_nonspam = []\n",
    "\n",
    "            for row in range(len(test_X)):\n",
    "                lst_nons = []\n",
    "                for col in range(12):\n",
    "                    pro = P_spam(test_X[row,:][col],dict_nonspam[col][0],dict_nonspam[col][2])\n",
    "                    lst_nons.append(pro+10**-300)\n",
    "                a= np.array(lst_nons)\n",
    "                product = np.log(pro_nonspam)+np.sum(np.log(a))\n",
    "                lst_nonspam.append(product)\n",
    "\n",
    "            # compare the probability for these two class\n",
    "            lst = []\n",
    "            for i in range(len(test_X)):\n",
    "                if lst_spam[i] >= lst_nonspam[i]:\n",
    "                    lst.append(1)\n",
    "                else:\n",
    "                    lst.append(0)\n",
    "\n",
    "            # calculate Accuracy \n",
    "            count = 0\n",
    "            for i in range(len(test_X)):\n",
    "                if test_y[i] == lst[i]:\n",
    "                    count += 1\n",
    "            Accuracy = count/len(test_X)\n",
    "            ls.append(Accuracy)\n",
    "            a = sum(ls)/len(ls)\n",
    "        print(\"k\\tAccuracy\")\n",
    "        print(\"{:.2f}\\t{:.3f}\".format(item, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k\tAccuracy\n",
      "5.00\t0.571\n",
      "k\tAccuracy\n",
      "10.00\t0.616\n",
      "k\tAccuracy\n",
      "50.00\t0.660\n",
      "k\tAccuracy\n",
      "100.00\t0.657\n",
      "k\tAccuracy\n",
      "500.00\t0.671\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation Naive on 12-paper-features-balanced dataset\n",
    "cross_valid_NB(\"12top_features_balanced.csv\", [5,10,50,100,500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GadId3Classifier:\n",
    "  def fit(self, input, output):\n",
    "    data = input.copy()\n",
    "    data[output.name] = output\n",
    "    self.tree = self.decision_tree(data, data, input.columns, output.name)\n",
    "\n",
    "  def predict(self, input):\n",
    "    # convert input data into a dictionary of samples\n",
    "    samples = input.to_dict(orient='records')\n",
    "    predictions = []\n",
    "\n",
    "    # make a prediction for every sample\n",
    "    for sample in samples:\n",
    "      predictions.append(self.make_prediction(sample, self.tree, 1.0))\n",
    "\n",
    "    return predictions\n",
    "\n",
    "  def entropy(self, attribute_column):\n",
    "    # find unique values and their frequency counts for the given attribute\n",
    "    values, counts = np.unique(attribute_column, return_counts=True)\n",
    "\n",
    "    # calculate entropy for each unique value\n",
    "    entropy_list = []\n",
    "\n",
    "    for i in range(len(values)):\n",
    "      probability = counts[i]/np.sum(counts)\n",
    "      entropy_list.append(-probability*np.log2(probability))\n",
    "\n",
    "    # calculate sum of individual entropy values\n",
    "    total_entropy = np.sum(entropy_list)\n",
    "\n",
    "    return total_entropy\n",
    "\n",
    "  def information_gain(self, data, feature_attribute_name, target_attribute_name):\n",
    "    # find total entropy of given subset\n",
    "    total_entropy = self.entropy(data[target_attribute_name])\n",
    "\n",
    "    # find unique values and their frequency counts for the attribute to be split\n",
    "    values, counts = np.unique(data[feature_attribute_name], return_counts=True)\n",
    "\n",
    "    # calculate weighted entropy of subset\n",
    "    weighted_entropy_list = []\n",
    "\n",
    "    for i in range(len(values)):\n",
    "      subset_probability = counts[i]/np.sum(counts)\n",
    "      subset_entropy = self.entropy(data.where(data[feature_attribute_name]==values[i]).dropna()[target_attribute_name])\n",
    "      weighted_entropy_list.append(subset_probability*subset_entropy)\n",
    "\n",
    "    total_weighted_entropy = np.sum(weighted_entropy_list)\n",
    "\n",
    "    # calculate information gain\n",
    "    information_gain = total_entropy - total_weighted_entropy\n",
    "\n",
    "    return information_gain\n",
    "\n",
    "  def decision_tree(self, data, orginal_data, feature_attribute_names, target_attribute_name, parent_node_class=None):\n",
    "    # base cases:\n",
    "    # if data is pure, return the majority class of subset\n",
    "    unique_classes = np.unique(data[target_attribute_name])\n",
    "    if len(unique_classes) <= 1:\n",
    "      return unique_classes[0]\n",
    "    # if subset is empty, ie. no samples, return majority class of original data\n",
    "    elif len(data) == 0:\n",
    "      majority_class_index = np.argmax(np.unique(original_data[target_attribute_name], return_counts=True)[1])\n",
    "      return np.unique(original_data[target_attribute_name])[majority_class_index]\n",
    "    # if data set contains no features to train with, return parent node class\n",
    "    elif len(feature_attribute_names) == 0:\n",
    "      return parent_node_class\n",
    "    # if none of the above are true, construct a branch:\n",
    "    else:\n",
    "      # determine parent node class of current branch\n",
    "      majority_class_index = np.argmax(np.unique(data[target_attribute_name], return_counts=True)[1])\n",
    "      parent_node_class = unique_classes[majority_class_index]\n",
    "\n",
    "      # determine information gain values for each feature\n",
    "      # choose feature which best splits the data, ie. highest value\n",
    "      ig_values = [self.information_gain(data, feature, target_attribute_name) for feature in feature_attribute_names]\n",
    "      best_feature_index = np.argmax(ig_values)\n",
    "      best_feature = feature_attribute_names[best_feature_index]\n",
    "\n",
    "      # create tree structure, empty at first\n",
    "      tree = {best_feature: {}}\n",
    "\n",
    "      # remove best feature from available features, it will become the parent node\n",
    "      feature_attribute_names = [i for i in feature_attribute_names if i != best_feature]\n",
    "\n",
    "      # create nodes under parent node\n",
    "      parent_attribute_values = np.unique(data[best_feature])\n",
    "      for value in parent_attribute_values:\n",
    "        sub_data = data.where(data[best_feature] == value).dropna()\n",
    "\n",
    "        # call the algorithm recursively\n",
    "        subtree = self.decision_tree(sub_data, orginal_data, feature_attribute_names, target_attribute_name, parent_node_class)\n",
    "\n",
    "        # add subtree to original tree\n",
    "        tree[best_feature][value] = subtree\n",
    "\n",
    "      return tree\n",
    "\n",
    "  def make_prediction(self, sample, tree, default=1):\n",
    "    # map sample data to tree\n",
    "    for attribute in list(sample.keys()):\n",
    "      # check if feature exists in tree\n",
    "      if attribute in list(tree.keys()):\n",
    "        try:\n",
    "          result = tree[attribute][sample[attribute]]\n",
    "        except:\n",
    "          return default\n",
    "\n",
    "        result = tree[attribute][sample[attribute]]\n",
    "\n",
    "        # if more attributes exist within result, recursively find best result\n",
    "        if isinstance(result, dict):\n",
    "          return self.make_prediction(sample, result)\n",
    "        else:\n",
    "          return result\n",
    "def cross_valid_DT(filename, k_ls):\n",
    "    sample = pd.read_csv(filename,header=0)\n",
    "\n",
    "    for item in k_ls:\n",
    "        from sklearn.model_selection import KFold\n",
    "        kf = KFold(n_splits=item)\n",
    "        trains = []\n",
    "        tests = []\n",
    "        for train,test in kf.split(sample):\n",
    "            trains.append(train)\n",
    "            tests.append(test)\n",
    "\n",
    "        mse = []\n",
    "        ls = []\n",
    "        for num in range(item):\n",
    "            cv_train = sample.iloc[trains[num]]\n",
    "            cv_test = sample.iloc[tests[num]]\n",
    "            train_X = np.ones(cv_train.shape[0]).reshape(-1,1)\n",
    "            test_X = np.ones(cv_test.shape[0]).reshape(-1,1)\n",
    "            train_y = cv_train.iloc[:,12].to_numpy().reshape(-1,1)\n",
    "            test_y = cv_test.iloc[:,12].to_numpy().reshape(-1,1)\n",
    "\n",
    "            for i in range (12):\n",
    "\n",
    "                cv_train_temp = cv_train.iloc[:,i].to_numpy()\n",
    "                cv_train_temp_mean = cv_train.iloc[:,i].mean()\n",
    "                cv_train_temp_std = cv_train.iloc[:,i].std()\n",
    "                cv_train_temp_stand= (cv_train_temp-cv_train_temp_mean)/cv_train_temp_std\n",
    "\n",
    "                train_X = np.append(train_X,cv_train_temp_stand.reshape(-1,1),axis=1) \n",
    "\n",
    "                cv_test_temp = cv_test.iloc[:,i].to_numpy()\n",
    "                cv_test_temp_stand = (cv_test_temp-cv_train_temp_mean)/cv_train_temp_std\n",
    "\n",
    "                test_X = np.append(test_X,cv_test_temp_stand.reshape(-1,1),axis=1)\n",
    "\n",
    "            train_X = train_X[:,1:13]\n",
    "            test_X = test_X[:,1:13]\n",
    "\n",
    "\n",
    "            train_X = pd.DataFrame(train_X)\n",
    "            train_y = pd.DataFrame(train_y)\n",
    "            test_X = pd.DataFrame(test_X)\n",
    "            test_y = pd.DataFrame(test_y)\n",
    "            train_X.name = 'train_X'\n",
    "            train_y.name = 'train_y'\n",
    "            test_X.name = 'test_X'\n",
    "            test_y.name = 'test_y'\n",
    "\n",
    "\n",
    "            from sklearn.metrics import accuracy_score\n",
    "            model = GadId3Classifier()\n",
    "            model.fit(train_X, train_y)\n",
    "\n",
    "            y_hat = model.predict(test_X)\n",
    "\n",
    "        #     print(accuracy_score(test_y, y_hat))\n",
    "            ls.append(accuracy_score(test_y, y_hat))\n",
    "            a = sum(ls)/len(ls)\n",
    "\n",
    "        print(\"k\\tAccuracy\")\n",
    "        print(\"{:.2f}\\t{:.3f}\".format(item, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k\tAccuracy\n",
      "5.00\t0.505\n",
      "k\tAccuracy\n",
      "10.00\t0.534\n",
      "k\tAccuracy\n",
      "50.00\t0.567\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-f2f049d59211>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Cross-validation Decision Tree on 12-paper-features-balanced dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcross_valid_DT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"12paper_features_balanced.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-89-c698e1ce7536>\u001b[0m in \u001b[0;36mcross_valid_DT\u001b[0;34m(filename, k_ls)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGadId3Classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-89-c698e1ce7536>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, input, output)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-89-c698e1ce7536>\u001b[0m in \u001b[0;36mdecision_tree\u001b[0;34m(self, data, orginal_data, feature_attribute_names, target_attribute_name, parent_node_class)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# call the algorithm recursively\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0msubtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morginal_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_attribute_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_attribute_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent_node_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# add subtree to original tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-89-c698e1ce7536>\u001b[0m in \u001b[0;36mdecision_tree\u001b[0;34m(self, data, orginal_data, feature_attribute_names, target_attribute_name, parent_node_class)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# call the algorithm recursively\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0msubtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morginal_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_attribute_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_attribute_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent_node_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# add subtree to original tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-89-c698e1ce7536>\u001b[0m in \u001b[0;36mdecision_tree\u001b[0;34m(self, data, orginal_data, feature_attribute_names, target_attribute_name, parent_node_class)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# call the algorithm recursively\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0msubtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morginal_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_attribute_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_attribute_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent_node_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# add subtree to original tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-89-c698e1ce7536>\u001b[0m in \u001b[0;36mdecision_tree\u001b[0;34m(self, data, orginal_data, feature_attribute_names, target_attribute_name, parent_node_class)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# call the algorithm recursively\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0msubtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morginal_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_attribute_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_attribute_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent_node_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# add subtree to original tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-89-c698e1ce7536>\u001b[0m in \u001b[0;36mdecision_tree\u001b[0;34m(self, data, orginal_data, feature_attribute_names, target_attribute_name, parent_node_class)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0;31m# determine information gain values for each feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0;31m# choose feature which best splits the data, ie. highest value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       \u001b[0mig_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minformation_gain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_attribute_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_attribute_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m       \u001b[0mbest_feature_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mig_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m       \u001b[0mbest_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_attribute_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_feature_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-89-c698e1ce7536>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0;31m# determine information gain values for each feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0;31m# choose feature which best splits the data, ie. highest value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       \u001b[0mig_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minformation_gain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_attribute_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_attribute_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m       \u001b[0mbest_feature_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mig_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m       \u001b[0mbest_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_attribute_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_feature_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-89-c698e1ce7536>\u001b[0m in \u001b[0;36minformation_gain\u001b[0;34m(self, data, feature_attribute_name, target_attribute_name)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m       \u001b[0msubset_probability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m       \u001b[0msubset_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_attribute_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_attribute_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m       \u001b[0mweighted_entropy_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset_probability\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msubset_entropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mwhere\u001b[0;34m(self, cond, other, inplace, axis, level, errors, try_cast)\u001b[0m\n\u001b[1;32m   9002\u001b[0m         \"\"\"\n\u001b[1;32m   9003\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9004\u001b[0;31m         return self._where(\n\u001b[0m\u001b[1;32m   9005\u001b[0m             \u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtry_cast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtry_cast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9006\u001b[0m         )\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_where\u001b[0;34m(self, cond, other, inplace, axis, level, errors, try_cast)\u001b[0m\n\u001b[1;32m   8867\u001b[0m                 \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblock_axis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8868\u001b[0m             )\n\u001b[0;32m-> 8869\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8870\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m                 \u001b[0;31m# GH#33357 fastpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m                 \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, copy, attrs)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_attrs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cross-validation Decision Tree on 12-paper-features-balanced dataset\n",
    "cross_valid_DT(\"12paper_features_balanced.csv\", [5,10,50,100,500])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
